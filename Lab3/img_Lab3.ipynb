{"cells":[{"cell_type":"markdown","metadata":{"id":"7lXXibDNXk8N"},"source":["# **3η ομαδική εργασία στη Τεχνολογία και Ανάλυση Εικόνων και Βίντεο**"]},{"cell_type":"markdown","metadata":{"id":"tEYVBW1oXpoY"},"source":["## **Στοιχεία μελών ομάδας**"]},{"cell_type":"markdown","metadata":{"id":"iFgZ9FbDXrJs"},"source":["Βασίλειος Βαρσαμής - 03118033\n","\n","Αλέξανδρος Κουλάκος - 03118144"]},{"cell_type":"markdown","metadata":{"id":"DfqHiQogXwII"},"source":["## **Θεωρητικό μέρος**"]},{"cell_type":"markdown","metadata":{"id":"uE3lY-vqgkc6"},"source":["### Ερώτηση 1"]},{"cell_type":"markdown","metadata":{"id":"WB2xE2cmX1Oj"},"source":["**Q**: Ποια είναι η διαφορά του bounding box και του anchor box στο YOLO;"]},{"cell_type":"markdown","metadata":{"id":"yVBMzvIXX8hT"},"source":["**A**: To bounding box καθορίζεται από τη διαδικασία εκπαίδευσης και επιλέγεται ως το καλύτερο για κάθε κλάση χρησιμοποιώντας τον non-max suppression αλγόριθμο. **Χρησιμοποιώντας μόνο bounding boxes κάθε grid cell μπορεί να πρόβλεψει το πολύ μία κλάση**, ωστόσο σε πολλές εικόνες υπάρχει επικάλυψη των εικονιζόμενων επιμέρους οντοτήτων (δηλαδή τα κέντρα τους ανήκουν στο ίδιο κελί) και έτσι η μέθοδος τα ταξινομεί σε μία κλάση, αγνοώντας τις υπόλοιπες. Η χρήση των anchor boxes έρχεται να λύσει το παραπάνω πρόβλημα με τον εξής τρόπο: ορίζουμε πριν την εκπαίδευση ένα σύνολο από bounding boxes δεδομένου ύψους και πλάτους, τα οποία θέλουμε να προσομοιάζουν το σχήμα των κλάσεων που προσδιορίζουν (anchor boxes). Το Νευρωνικό Δίκτυο προβλέπει κάποιo offset από τις διαστάσεις του anchor box για τις διαστάσεις του υποψήφιου κουτιού και με βάση μια συνάρτηση απώλειας κάνει σταδιακά καλύτερη αυτήν την πρόβλεψη. Με αυτόν τον τρόπο καθίσταται δυνατή η αναγνώριση πολλών αντικειμένων, που έχουν διαφορετική κλίμακα ή/και που επικαλύπτονται."]},{"cell_type":"markdown","metadata":{"id":"V-yURI0VgoRO"},"source":["### Ερώτηση 2"]},{"cell_type":"markdown","metadata":{"id":"RdmvGRLdYAJ8"},"source":["**Q**: Ποιες θα είναι οι διαστάσεις του πίνακα εξόδου (πρόβλεψης) y_hat του αλγορίθμου YOLO\n","θεωρώντας ότι έχουμε δυο anchor boxes και 3 κλάσεις; Αναφέρετε επίσης το ρόλο για κάθε\n","στοιχείο του πίνακα αυτού."]},{"cell_type":"markdown","metadata":{"id":"BtskIb8yX_qX"},"source":["**A**: Αν θέσουμε $Β$ το πλήθος των anchor boxes, $Ν$ τη διάσταση του grid και $C$ το πλήθος των κλάσεων προς ταξινόμηση, ο τένσορας της εξόδου του αλγορίθμου YOLO έχει διαστάσεις $ N × N × B\\cdot(C + 5) $, διότι για κάθε grid cell θα έχουμε ένα διάνυσμα $y$ που θα αποτελείται από $ B\\cdot(C + 5) $ θέσεις. Συγκεκριμένα, οι θέσεις του διανύσματος $y$ έχουν ως εξής:\n"," \n","•\t1 για το detection probability $p_c$ (δηλαδή την πιθανότητα να βρούμε μια συγκεκριμένη κλάση μέσα στο εν λόγω grid cell)\n","\n","•\t4 για τις συντεταγμένες του bounding box\n","\n","•\t$C$ για τις προς αναγνώριση κλάσεις\n","\n","Και τα παραπάνω $B$ φορές, μία για κάθε anchor box.\n","\n","Επομένως τελικά, αν περιοριστούμε σε ένα grid cell για $ B = 2 $ και $ C = 3 $ έχουμε στην έξοδο διάνυσμα με διαστάσεις $ 1 × 16 $.\n"]},{"cell_type":"markdown","metadata":{"id":"Dkij_myxgqyK"},"source":["### Ερώτηση 3"]},{"cell_type":"markdown","metadata":{"id":"E7f-YLy7YFQm"},"source":["**Q**: Εξηγήστε συνοπτικά την έννοια της μεθόδου Non-max suppression. Σε τι χρησιμεύει;"]},{"cell_type":"markdown","metadata":{"id":"DrnhF6HnYFI8"},"source":["**A**: Η non-max suppression είναι μια τεχνική που χρησιμοποιείται σε προβλήματα υπολογιστικής όρασης και αποτελείται από μία κλάση αλγορίθμων που επιλέγουν μία οντότητα (εδώ bounding boxes) από πολλές υποψήφιες. Η επιλογή αυτή γίνεται με κάποια κριτήρια που στην περίπτωση μας είναι δύο: α) η πιθανότητα ανίχνευσης, β) κάποια μετρική επικάλυψης (εδώ η IoU).\n","\n","Η ιδέα της τεχνικής αυτής που περιγράφεται παρακάτω είναι εξαιρετικά απλή. Ο αλγόριθμος εξελίσσεται σε δύο βήματα:\n","\n","1)\tΑπό την λίστα των προβλέψεων (έστω $P$) για τα bounding boxes παίρνουμε αυτή που έχει την μεγαλύτερη τιμή $p_c$ και την τοποθετούμε στην τελική λίστα (έστω $Κ$).\n","\n","2)\tΓια όλες τις εναπομείνουσες προβλέψεις στην $P$ υπολογίζουμε την IoU με αυτήν που βάλαμε στη λίστα $Κ$ και απορρίπτουμε όλες αυτές που έχουν IoU μεγαλύτερο από κάποιο threshold (π.χ 0.5) και συνεχίζουμε από το βήμα 1 μέχρι να αδειάσει η λίστα $P$.\n"," \n","Διαισθητικά ο παραπάνω αλγόριθμος κάνει το παρακάτω (για κάθε κλάση):\n","\n","Κρατάει την καλύτερη πρόβλεψη (αυτήν με το μεγαλύτερο $p_c$), δηλαδή την maximal, και κάθε λύση που δεν είναι maximal, αλλά επικαλύπτεται ποσοστιαία παραπάνω από κάποιο ορισμένο κατώφλι με αυτήν που κρατήσαμε, την απορρίπτουμε (δηλαδή suppress the non - maximal). Πράγματι, είναι λογικό να την απορρίπτουμε, γιατί έχουμε ήδη μία πρόβλεψη (αυτή που κρατήσαμε) με καλύτερη τιμή του $p_c$ και που μοιάζει πολύ χωρικά με αυτή που απορρίπτουμε, εφόσον έχει IoU μεγαλύτερο από κάποιο κατώφλι. Άρα, και στις δύο παραμέτρους η πρόβλεψη που απορρίψαμε “χάνει” απέναντι σε αυτήν που κρατήσαμε.\n"]},{"cell_type":"markdown","metadata":{"id":"vhGIJRJPgt9N"},"source":["### Ερώτηση 4"]},{"cell_type":"markdown","metadata":{"id":"QRmPruyBYE_s"},"source":["**Q**: Στον αλγόριθμο SORT το ταίριασμα (matching) των αντικειμένων με ποια μέθοδο γίνεται;\n","Αναφέρετε ένα παράδειγμα μετρικής για το bounding boxes distance.\n"]},{"cell_type":"markdown","metadata":{"id":"vKE0ezxnYE0g"},"source":["**A**: Στον αλγόριθμο SORT το ταίριασμα (matching) των αντικειμένων γίνεται με τη μέθοδο του Hungarian algorithm. Ένα παράδειγμα μετρικής για το bounding boxes distance αποτελεί η ΙοU (Intersection over Union) που στην ουσία ταυτίζεται με την ομοιότητα Jaccard για δύο σύνολα $ A, B $ που ορίζεται ως: $J = \\frac{|Α \\cap Β|}{|A \\cup B|}$, όπου εδώ τα $ Α, B $ είναι τα σύνολα των pixels που καταλαμβάνουν τα δύο bounding boxes."]},{"cell_type":"markdown","metadata":{"id":"axjiuEpFgv6y"},"source":["### Ερώτηση 5"]},{"cell_type":"markdown","metadata":{"id":"AhPhlZv4YEnj"},"source":["**Q**: Πώς ορίζεται η μέθοδος του tracking-by-detection και σε ποιο component αυτής της\n","μεθόδου χρησιμοποιείται βαθιά μάθηση (deep learning);"]},{"cell_type":"markdown","metadata":{"id":"3ayzY21-YEO1"},"source":["**A**: Η μέθοδος του tracking by detection ορίζεται ως εξής: έχουμε κάποια καρέ εικόνας ενός βίντεο. Πάνω σε αυτές, εφαρμόζουμε έναν ανιχνευτή ο οποίος δίνει πιθανά bounding boxes που περιέχουν στο εσωτερικό τους κάποια κλάση (στην εργασία που δίνεται πρόκειται για πεζούς). Αυτό γίνεται για κάθε καρέ και χρησιμοποιώντας CNN αρχιτεκτονικές (όπως το YOLO ή άλλα FrCNNs). Έπειτα, με την χρήση φίλτρων Kalman καθώς και του Hungarian αλγορίθμου, γίνεται το tracking, δηλαδή το data association (σύνδεση των ανεξάρτητων ανιχνεύσεων κάθε καρέ) για να πάρουμε την πλήρη τροχιά. To component της παραπάνω μεθόδου, όπως προαναφέραμε, στο οποίο χρησιμοποιείται deep learning είναι το detection."]},{"cell_type":"markdown","metadata":{"id":"v29IXsnJYqcX"},"source":["## **Εργαστηριακό μέρος**"]},{"cell_type":"markdown","metadata":{"id":"t8eNV308Ytuy"},"source":["### **Προετοιμασία των απαραίτητων πακέτων/βιβλιοθηκών**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19889,"status":"ok","timestamp":1654525262170,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"OpGVemg6ZQyo","outputId":"58c8bb25-f1e5-4bca-90e8-091e01c5f78a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.7/dist-packages (1.5.0)\n","Requirement already satisfied: torchvision==0.6.0 in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0) (1.21.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: filterpy==1.4.5 in /usr/local/lib/python3.7/dist-packages (1.4.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from filterpy==1.4.5) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from filterpy==1.4.5) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy==1.4.5) (3.2.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (1.4.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->filterpy==1.4.5) (4.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy==1.4.5) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-image==0.17.2 in /usr/local/lib/python3.7/dist-packages (0.17.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (2.4.1)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (3.2.2)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (1.4.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (2021.11.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (2.6.3)\n","Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (1.21.6)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (7.1.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (4.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.17.2) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: lap==0.4.0 in /usr/local/lib/python3.7/dist-packages (0.4.0)\n"]}],"source":["import sys\n","\n","!{sys.executable} -m pip install torch==1.5.0 torchvision==0.6.0\n","!{sys.executable} -m pip install matplotlib\n","!{sys.executable} -m pip install pillow\n","!{sys.executable} -m pip install filterpy==1.4.5\n","!{sys.executable} -m pip install scikit-image==0.17.2\n","!{sys.executable} -m pip install lap==0.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2171,"status":"ok","timestamp":1654525264326,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"24HtBgt1ZUTs","outputId":"b326f2bd-2f78-44fc-8ff4-a6d440958b8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive filesystem\n","# to access necessary python modules\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1654525264327,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"L3VhLPp2ZbZX","outputId":"a1736f9b-ea60-4fa2-b82b-d648e0cd8201"},"outputs":[{"name":"stdout","output_type":"stream","text":["['models.py', 'sort.py', 'group_92.mp4', 'config', 'images', 'utils', '__pycache__', 'Object_Detection_and_Tracking.ipynb', 'img_Lab3.ipynb']\n"]}],"source":["import os \n","\n","# Absolute path to working directory\n","path = '/content/drive/My Drive/Image_Video_Technology/Tasks/Task_3/Lab_motion_tracking_exercise/'\n","\n","# Interpreter will search in the working directory for the necessary modules\n","sys.path.insert(0, path)\n","\n","# Check that all necessary files/folders exist\n","print(os.listdir(path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBWg7vsxZgVx"},"outputs":[],"source":["from models import *\n","from utils import *\n","from sort import *\n","\n","import time, datetime, random\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import numpy as np\n","from PIL import Image\n","\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1654525265754,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"KjmMR74Qazx0","outputId":"64f0a54d-4110-4eb4-a17b-d31cad6684fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current working directory: /content\n"]}],"source":["print(\"Current working directory:\", os.getcwd())"]},{"cell_type":"markdown","metadata":{"id":"VpveEnZebGm3"},"source":["### Κατασκευή των βοηθητικών συναρτήσεων"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CckKasida7b8"},"outputs":[],"source":["# Function that uses a pre-trained Neural Network to make image detections \n","# for specified values of conf_thres and nms_thres.\n","\n","def detect_image(img, conf_thres, nms_thres):\n","    # scale and pad image\n","    ratio = min(img_size / img.size[0], img_size / img.size[1])\n","    imw = round(img.size[0] * ratio)\n","    imh = round(img.size[1] * ratio)\n","\n","    img_transforms = transforms.Compose([ \n","                                         transforms.Resize((imh, imw)),\n","                                         transforms.Pad((max(int((imh - imw) / 2), 0), max(int((imw - imh) / 2), 0), \n","                                                         max(int((imh - imw) / 2), 0), max(int((imw - imh) / 2), 0)), \n","                                                        (128, 128, 128)),\n","                                         transforms.ToTensor(),\n","                                          ])\n","    # convert image to Tensor\n","    image_tensor = img_transforms(img).float()\n","    image_tensor = image_tensor.unsqueeze_(0)\n","    input_img = Variable(image_tensor.type(Tensor))\n","    # run inference on the model and get detections\n","    with torch.no_grad():\n","        detections = model(input_img)\n","        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n","    \n","    return detections[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v74vV_ipe72o"},"outputs":[],"source":["# Function that uses a pre-trained Neural Network to make and plot detections in a video (frame by frame) \n","# for specified values of conf_thres and nms_thres.\n","\n","def object_tracking(conf_thres, nms_thres):\n","  vid = cv2.VideoCapture(videopath)\n","  mot_tracker = Sort() \n","  frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n","\n","  for ii in range(int(frames)):\n","      ret, frame = vid.read()\n","      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      pilimg = Image.fromarray(frame)\n","      detections = detect_image(pilimg, conf_thres, nms_thres)\n","\n","      img = np.array(pilimg)\n","      pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n","      pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n","      unpad_h = img_size - pad_y\n","      unpad_w = img_size - pad_x\n","\n","      if (detections is not None) and (ii % 20 == 0):\n","          print(\"ii\", ii)\n","          print(detections.shape)\n","          tracked_objects = mot_tracker.update(detections.cpu())\n","          print(tracked_objects.shape)\n","\n","          unique_labels = detections[:, -1].cpu().unique()\n","          n_cls_preds = len(unique_labels)\n","          for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n","              box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n","              box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n","              y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n","              x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n","\n","              color = colors[int(obj_id) % len(colors)]\n","              color = [i * 255 for i in color]\n","              cls = classes[int(cls_pred)]\n","              cv2.rectangle(frame, (x1, y1), (x1 + box_w, y1 + box_h), color, 4)\n","              cv2.rectangle(frame, (x1, y1 - 35), (x1 + len(cls) * 19 + 60, y1), color, -1)\n","              cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n","      \n","      if ii % 20 == 0:\n","        fig=plt.figure(figsize=(12, 8))\n","        plt.title(\"Video Stream\")\n","        plt.imshow(frame)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x2_vkkzEkYFd"},"source":["### Πειραματισμός με τις υπερπαραμέτρους `conf_thres` και `nms_thres`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1654525266112,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"6bSSL-8tmIgm","outputId":"0bdfd991-6e3d-43ae-a1ae-1b9e3c84e4a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['random']\n","`%matplotlib` prevents importing * from pylab and numpy\n","  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"]}],"source":["%pylab inline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4714,"status":"ok","timestamp":1654525270822,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"2036aTBZmrxq","outputId":"2c420e26-8992-4bec-f988-427446589051"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"]}],"source":["# Load parameters of the Neural Network\n","config_path = path + 'config/yolov3.cfg'\n","\n","# Load weights of the pre-trained model YOLOv3 on COCO dataset\n","weights_path = path + 'config/yolov3.weights'\n","\n","# Load classes for detection (80 classes in total)\n","class_path = path + 'config/coco.names'\n","\n","img_size = 416\n","\n","# Load DarkNet model and weights\n","model = Darknet(config_path, img_size=img_size)\n","model.load_weights(weights_path)\n","model.cuda()\n","model.eval()\n","classes = utils.load_classes(class_path)\n","Tensor = torch.cuda.FloatTensor\n","\n","# Absolute path to group video\n","videopath = path + 'group_92.mp4'\n","\n","cmap = plt.get_cmap('tab20b')\n","colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]"]},{"cell_type":"markdown","metadata":{"id":"HKq-hQsEW6bD"},"source":["Λόγω του πολύ μεγάλου output (το βίντεο συντίθεται από 300 καρέ), επιλέγουμε να εμφανίζουμε τα καρέ ανά 20 (δηλαδή, σε κάθε πειραματισμό τυπώνονται $ 300/20 = 15 $ εικόνες). Πράγματι, αυτή η δειγματοληψία είναι πρακτική, διότι τα εμφανιζόμενα καρέ είναι τελείως αντιπροσωπευτικά όσον αφορά τα συμπεράσματα που αναλύονται παρακάτω."]},{"cell_type":"markdown","metadata":{"id":"geM7KldqlRn3"},"source":["#### conf_thres = 0.3, nms_thres = 0.4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1HFxv0hPvZC6_S6wrryE0zDc9XMlFcGto"},"executionInfo":{"elapsed":81875,"status":"ok","timestamp":1654525352688,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"WsbjWYkqfNVk","outputId":"e9f165a7-d5e4-4619-c47e-e01d0aed5fc4"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["object_tracking(0.3, 0.4)"]},{"cell_type":"markdown","metadata":{"id":"7mhPbP-tlnBD"},"source":["#### conf_thres = 0.9, nms_thres = 0.4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"169IdXfKhYa5G3qdzcAUN7J8pEXJ1ZBj6"},"executionInfo":{"elapsed":67478,"status":"ok","timestamp":1654525420149,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"-u4FWs00lj73","outputId":"c629e8b9-af45-445f-9cd2-9d1faa8963b3"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["object_tracking(0.9, 0.4)"]},{"cell_type":"markdown","metadata":{"id":"puIkorrcloIl"},"source":["#### conf_thres = 0.8, nms_thres = 0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"10YrqgPn3AYkoCuJPEelP3f-AR2oRz8Of"},"executionInfo":{"elapsed":68405,"status":"ok","timestamp":1654525488517,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"JuWtZpPjl3C_","outputId":"51aaf9f4-d018-4e78-a7ae-b65a1691b276"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["object_tracking(0.8, 0.3)"]},{"cell_type":"markdown","metadata":{"id":"rpdFX4c3ln27"},"source":["#### conf_thres = 0.8, nms_thres = 0.9"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1tw7G5JWoWZZ7T6JfNgWI4Kuk_Q5i1427"},"executionInfo":{"elapsed":69421,"status":"ok","timestamp":1654525557922,"user":{"displayName":"Αλέξανδρος Κουλάκος","userId":"04432830327029943355"},"user_tz":-180},"id":"6e2UAFpGl4GN","outputId":"9bbc7a4c-4f0d-42a3-e269-3d9f426521d2"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["object_tracking(0.8, 0.9)"]},{"cell_type":"markdown","metadata":{"id":"I-gXSGzr0AVf"},"source":["#### Συμπεράσματα"]},{"cell_type":"markdown","metadata":{"id":"_gxlrZrj9Spp"},"source":["##### Confidence threshold"]},{"cell_type":"markdown","metadata":{"id":"M8fpVIes0HA4"},"source":["Bounding boxes για τα οποία ισχύει ότι `conf_score < conf_thres` (δηλαδή η πιθανότητα ανίχνευσης αντικειμένου είναι μικρότερη από την τιμή κατωφλίου που ορίζουμε εμείς) δε λαμβάνονται υπόψη στο πρόβλημα του image detection. Έτσι, φαίνεται ότι το μέγεθος `conf_thres` διαδραματίζει καθοριστικό ρόλο όσον αφορά το σύνολο των bounding boxes που θεωρούμε ότι ανιχνεύουν ικανοποιητικά κάποιο αντικείμενο, δηλαδή διαφορετικές τιμές του `conf_thres` καθορίζουν την ποιότητα και την ορθότητα της ανίχνευσης. Πράγματι, αυτή η διαπίστωση δικαιολογείται από τα αποτελέσματα της προσομοίωσης. Συγκεκριμένα, παρατηρούμε ότι για μικρές τιμές του `conf_thres` (π.χ. `conf_thres = 0.3`), σε κάθε καρέ του βίντεο επισημαίνονται πάρα πολλά bounding boxes (σε βαθμό που τα καθιστά έως και δυσδιάκριτα μεταξύ τους) και επίσης πολλά bounding boxes κάνουν λανθασμένες προβλέψεις (βλ. καρέ υπ' αριθμόν 20, 80 και 100), που σημαίνει ότι λόγω της χαμηλής τιμής `conf_thres` που ορίσαμε επιτρέπουμε σε πολλά bounding boxes να ανιχνεύουν αντικείμενα, ωστόσο όχι και τόσο ικανοποιητικά. Αντίθετα, παρατηρούμε ότι για μεγάλες τιμές του `conf_thres` (π.χ. `conf_thres = 0.9`), σε κάθε καρέ του βίντεο επισημαίνονται πλέον πολύ λίγα bounding boxes τα οποία κάνουν ολόσωστες προβλέψεις, που σημαίνει ότι λόγω της υψηλής τιμής `conf_thres` που ορίσαμε επιτρέπουμε σε λίγα bounding boxes να ανιχνεύουν αντικείμενα και μάλιστα με ιδιαίτερα ικανοποιητικό τρόπο. \n","\n","**Συνεπώς, μέσα από διάφορους πειραματισμούς, είναι καίριο να προσδιορίσουμε εκείνη την τιμή του `conf_thres` η οποία οδηγεί στην ύπαρξη πολλών bounding boxes (δηλαδή στην ανίχνευση πολλών αντικειμένων), αλλά ταυτόχρονα με καλή ακρίβεια. Συνήθως, μια καλή τέτοια τιμή είναι `conf_thres = 0.7` ή `conf_thres= 0.8`.**"]},{"cell_type":"markdown","metadata":{"id":"f199j6hl9bn7"},"source":["##### Non-max Suppression threshold"]},{"cell_type":"markdown","metadata":{"id":"CxID6UI19hTS"},"source":["Έστω ότι βρισκόμαστε στο στάδιο non-max suppression (κατά το οποίο σκοπός είναι να απαλείψουμε bounding boxes που αναφέρονται στο ίδιο αντικείμενο) και έχουμε επιλέξει το bounding box με το υψηλότερο `conf_score`. Τότε, βρίσκουμε το `IoU` αυτού του box με όλα τα υπόλοιπα bounding boxes και απορρίπτουμε εκείνα τα boxes για τα οποία ισχύει `IoU > nms_thres`. Συνεπώς, προκύπτει ότι όσο μικρότερο είναι η τιμή `nms_thres` που θέτουμε (δηλαδή όσο πιο \"αυστηροί\" είμαστε), τόσο πιο λίγα bounding boxes που προσδιορίζουν το ίδιο αντικείμενο γίνονται αποδεκτά, ενώ όσο μεγαλύτερη είναι η τιμή `nms_thres` που θέτουμε (δηλαδή όσο πιο \"ελαστικοί\" είμαστε), τόσο πιο πολλά bounding boxes που προσδιορίζουν το ίδιο αντικείμενο γίνονται αποδεκτά. Πράγματι, αυτή η διαπίστωση δικαιολογείται από τα αποτελέσματα της προσομοίωσης. Συγκεκριμένα, παρατηρούμε ότι για μικρές τιμές του `nms_thres` (π.χ. `nms_thres = 0.3`), στα περισσότερα καρέ του βίντεο κάθε αντικείμενο πλαισιώνεται (ανιχνεύεται) από ένα και μόνο bounding box, ενώ για μεγάλες τιμές του `nms_thres` (π.χ. `nms_thres = 0.9`) στα περισσότερα καρέ υπάρχουν αντικείμενα (π.χ. bus) που πλαισιώνονται από παραπάνω από ένα bounding boxes.\n","\n","**Συνεπώς, προκύπτει ότι είναι βολικό και αποδοτικό να επιλέγουμε μικρές τιμές για το nms_thres, έτσι ώστε να είναι αποτελεσματική η απαλοιφή bounding boxes που ανιχνεύουν το ίδιο αντικείμενο. Συνήθως, καλές τέτοιες τιμές είναι `nms_thres = 0.3` και `nms_thres = 0.4`.**"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"img_Lab3.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.8"},"vscode":{"interpreter":{"hash":"47ca7b06c7422198049f765dc9ab9559bb1b3c328d4538cd7e023870867a1c7e"}}},"nbformat":4,"nbformat_minor":0}
